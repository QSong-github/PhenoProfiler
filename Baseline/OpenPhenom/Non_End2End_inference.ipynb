{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5edcb7d2-53dc-4170-9f2f-619c0da0ae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef5399",
   "metadata": {},
   "source": [
    "# 1. Test BBBC022 dataset in non end to end pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd596f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "from huggingface_mae import MAEModel\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "\n",
    "# PNG in /data/boom/cpg0019/broad/training_images/BBBC022/20585/A01/1/ .png   : 960 x 160, 160 x 5 = 800\n",
    "# save in /data/boom/cpg0019/broad/workspace_dl/embeddings/105281_zenodo7114558/BBBC022/20585/A01/1/OpenPhenom_embedding.npy\n",
    "\n",
    "class NewnoendDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    noendDataset(image_path = \"/data/boom/cpg0019/broad/\",\n",
    "               embedding_path = \"/data/boom/cpg0019/broad/workspace_dl/embeddings/105281_zenodo7114558/\",\n",
    "            #    CSV_path = \"/data/boom/cpg0019/broad/workspace_dl/metadata/sc-metadata-fil.csv\")\n",
    "    '''\n",
    "    def __init__(self, image_path, embedding_path, CSV_path):\n",
    "\n",
    "        self.image_path = image_path\n",
    "        self.embedding_path = embedding_path\n",
    "        self.CSV_file = pd.read_csv(CSV_path)\n",
    "        \n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.encode_labels()\n",
    "\n",
    "    def encode_labels(self):\n",
    "        if 'Treatment' in self.CSV_file.columns:\n",
    "            column_name = 'Treatment'\n",
    "        elif 'pert_name' in self.CSV_file.columns:\n",
    "            column_name = 'pert_name'\n",
    "        else:\n",
    "            raise ValueError(\"CSV file must contain either 'Treatment' or 'pert_name' column\")\n",
    "\n",
    "        self.CSV_file['encoded_labels'] = self.label_encoder.fit_transform(self.CSV_file[column_name])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        # 获取图像位置，然后读取，然后按照坐标拆分，得到5张图象，然后叠加\n",
    "        img_path = self.image_path + str(self.CSV_file.loc[idx, 'Image_Name'])[6:]\n",
    "        All_img = Image.open(img_path)\n",
    "        # print(img_path, All_img.size)\n",
    "\n",
    "        # 分割大图像为6个子图像，每个子图像的尺寸为（160，160）\n",
    "        sub_images = []\n",
    "        for i in range(6):\n",
    "            left = i * 160\n",
    "            upper = 0\n",
    "            right = left + 160\n",
    "            lower = upper + 160\n",
    "            sub_image = All_img.crop((left, upper, right, lower))\n",
    "            sub_images.append(sub_image)\n",
    "        \n",
    "        # 按照通道叠加前面5张子图像在一起\n",
    "        combined_image = np.stack(sub_images[:5], axis=0)\n",
    "        # print(combined_image.shape)\n",
    "        resized_image = resize(combined_image, (5, 256, 256), anti_aliasing=True)\n",
    "\n",
    "        # embedding\n",
    "        # /data/boom/cpg0019/broad/workspace_dl/embeddings/105281_zenodo7114558/BBBC022/20585/A01/1\n",
    "        path = os.path.dirname(self.CSV_file.loc[idx, 'Image_Name'][22:])\n",
    "        embedding_path = os.path.join(self.embedding_path, str(path))\n",
    "        # print(\"embedding_path:\", self.embedding_path, path, embedding_path)\n",
    "        # with open(embedding_path, \"rb\") as data:\n",
    "        #     info = np.load(data)\n",
    "        #     cells = np.array(np.copy(info[\"features\"]))\n",
    "        #     embedding = cells[~np.isnan(cells).any(axis=1)]\n",
    "        #     # embedding = np.median(embedding, axis=0)\n",
    "        #     # print(idx, embedding.shape)\n",
    "\n",
    "        #     embedding = embedding[idx]\n",
    "            # print(idx, embedding.shape)\n",
    "\n",
    "        item['image'] = torch.tensor(resized_image).float()  # torch.Size([5, 448, 448]) \n",
    "        item['embedding'] = embedding_path\n",
    "        # print(item['embedding'].shape, item['image'].shape)\n",
    "        \n",
    "        # encoded_labels = self.CSV_file.loc[idx, 'encoded_labels']\n",
    "        # item['class'] = torch.tensor(encoded_labels).long()\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.CSV_file.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41a6555e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Collection</th>\n",
       "      <th>Metadata_Plate</th>\n",
       "      <th>Metadata_Well</th>\n",
       "      <th>Metadata_Site</th>\n",
       "      <th>Nuclei_Location_Center_X</th>\n",
       "      <th>Nuclei_Location_Center_Y</th>\n",
       "      <th>Image_Name</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>Treatment_Type</th>\n",
       "      <th>Control</th>\n",
       "      <th>Cell_line</th>\n",
       "      <th>LeaveReplicatesOut</th>\n",
       "      <th>LeaveCellsOut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BBBC022</td>\n",
       "      <td>20585</td>\n",
       "      <td>A01</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>28</td>\n",
       "      <td>../../training_images/BBBC022/20585/A01/1/1583...</td>\n",
       "      <td>BRD-K98763141</td>\n",
       "      <td>Compound</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>U2OS</td>\n",
       "      <td>Training</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BBBC022</td>\n",
       "      <td>20585</td>\n",
       "      <td>A01</td>\n",
       "      <td>1</td>\n",
       "      <td>444</td>\n",
       "      <td>33</td>\n",
       "      <td>../../training_images/BBBC022/20585/A01/1/1583...</td>\n",
       "      <td>BRD-K98763141</td>\n",
       "      <td>Compound</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>U2OS</td>\n",
       "      <td>Training</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BBBC022</td>\n",
       "      <td>20585</td>\n",
       "      <td>A01</td>\n",
       "      <td>1</td>\n",
       "      <td>573</td>\n",
       "      <td>52</td>\n",
       "      <td>../../training_images/BBBC022/20585/A01/1/1583...</td>\n",
       "      <td>BRD-K98763141</td>\n",
       "      <td>Compound</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>U2OS</td>\n",
       "      <td>Training</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BBBC022</td>\n",
       "      <td>20585</td>\n",
       "      <td>A01</td>\n",
       "      <td>1</td>\n",
       "      <td>304</td>\n",
       "      <td>62</td>\n",
       "      <td>../../training_images/BBBC022/20585/A01/1/1583...</td>\n",
       "      <td>BRD-K98763141</td>\n",
       "      <td>Compound</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>U2OS</td>\n",
       "      <td>Training</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BBBC022</td>\n",
       "      <td>20585</td>\n",
       "      <td>A01</td>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>66</td>\n",
       "      <td>../../training_images/BBBC022/20585/A01/1/1583...</td>\n",
       "      <td>BRD-K98763141</td>\n",
       "      <td>Compound</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>U2OS</td>\n",
       "      <td>Training</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Collection  Metadata_Plate Metadata_Well  Metadata_Site  \\\n",
       "0    BBBC022           20585           A01              1   \n",
       "1    BBBC022           20585           A01              1   \n",
       "2    BBBC022           20585           A01              1   \n",
       "3    BBBC022           20585           A01              1   \n",
       "4    BBBC022           20585           A01              1   \n",
       "\n",
       "   Nuclei_Location_Center_X  Nuclei_Location_Center_Y  \\\n",
       "0                       128                        28   \n",
       "1                       444                        33   \n",
       "2                       573                        52   \n",
       "3                       304                        62   \n",
       "4                       143                        66   \n",
       "\n",
       "                                          Image_Name      Treatment  \\\n",
       "0  ../../training_images/BBBC022/20585/A01/1/1583...  BRD-K98763141   \n",
       "1  ../../training_images/BBBC022/20585/A01/1/1583...  BRD-K98763141   \n",
       "2  ../../training_images/BBBC022/20585/A01/1/1583...  BRD-K98763141   \n",
       "3  ../../training_images/BBBC022/20585/A01/1/1583...  BRD-K98763141   \n",
       "4  ../../training_images/BBBC022/20585/A01/1/1583...  BRD-K98763141   \n",
       "\n",
       "  Treatment_Type    Control Cell_line LeaveReplicatesOut LeaveCellsOut  \n",
       "0       Compound  Treatment      U2OS           Training      Training  \n",
       "1       Compound  Treatment      U2OS           Training      Training  \n",
       "2       Compound  Treatment      U2OS           Training      Training  \n",
       "3       Compound  Treatment      U2OS           Training      Training  \n",
       "4       Compound  Treatment      U2OS           Training      Training  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = pd.read_csv(os.path.join('/data/boom/cpg0019/broad/workspace_dl/metadata/sc-bbbc022.csv'))\n",
    "meta_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ad182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 定义超参数\n",
    "save_path = \"../bbbc022/\"\n",
    "\n",
    "# 读取meta文件\n",
    "meta_data = pd.read_csv(os.path.join('/data/boom/cpg0019/broad/workspace_dl/metadata/sc-bbbc022.csv'))\n",
    "\n",
    "# 针对全部图像，逐个读取，逐个处理单个图像, 返回读取好的图像\n",
    "def image_process(img_paths):\n",
    "    all_img = []\n",
    "    for img_path in img_paths:\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            # 分割大图像为6个子图像，每个子图像的尺寸为（160，160）\n",
    "            sub_images = []\n",
    "            for i in range(6):\n",
    "                left = i * 160\n",
    "                upper = 0\n",
    "                right = left + 160\n",
    "                lower = upper + 160\n",
    "                sub_image = img.crop((left, upper, right, lower))\n",
    "                sub_images.append(sub_image)\n",
    "            \n",
    "            # 按照通道叠加前面5张子图像在一起\n",
    "            combined_image = np.stack(sub_images[:5], axis=0)\n",
    "            resized_image = resize(combined_image, (5, 224, 224), anti_aliasing=True)\n",
    "            all_img.append(resized_image)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {img_path}\")\n",
    "            continue\n",
    "    \n",
    "    # 拼接all_img的所有数据，成为维度(B, 5, 224, 224)\n",
    "    if all_img:\n",
    "        all_img_tensor = torch.tensor(np.array(all_img)).float()\n",
    "        return all_img_tensor\n",
    "    else:\n",
    "        return torch.empty(0)\n",
    "\n",
    "# 加载模型\n",
    "MODEL_PATH = \"recursionpharma/OpenPhenom\"\n",
    "model = MAEModel.from_pretrained(MODEL_PATH).cuda()\n",
    "# img_embeddings = get_image_embeddings(MODEL_PATH, model, batch_size=1)  # change batch_size to fit your device\n",
    "# features = img_embeddings.cpu().numpy()\n",
    "\n",
    "model.eval()\n",
    "print(\"Finished loading model\")\n",
    "\n",
    "def model_eval(batch_imgs):\n",
    "    model = MAEModel.from_pretrained(MODEL_PATH).cuda()\n",
    "    model.return_channelwise_embeddings = False\n",
    "    image_embeddings = model.predict(batch_imgs.cuda())\n",
    "    \n",
    "    return image_embeddings\n",
    "\n",
    "def get_png_filenames(directory):\n",
    "    # 支持的图像文件扩展名\n",
    "    image_extension = '.png'\n",
    "    image_filenames = []\n",
    "\n",
    "    try:\n",
    "        # 遍历目录中的文件（不包括子目录）\n",
    "        for file in os.listdir(directory):\n",
    "            # 检查文件扩展名是否为PNG\n",
    "            if file.lower().endswith(image_extension):\n",
    "                image_filenames.append(os.path.join(directory, file))\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "        # print(f\"Directory not found: {directory}\")\n",
    "    \n",
    "    return image_filenames\n",
    "\n",
    "# 存储所有site的处理结果\n",
    "features = []\n",
    "\n",
    "# 遍历每个plate\n",
    "for plate in tqdm(meta_data[\"Metadata_Plate\"].unique()):\n",
    "    print(plate)\n",
    "    # if plate.astype(str) in ['20585', '20586', '20589']:\n",
    "    #     continue\n",
    "    # print(plate)\n",
    "    m1 = meta_data[\"Metadata_Plate\"] == plate\n",
    "    wells = meta_data[m1][\"Metadata_Well\"].unique()\n",
    "    \n",
    "    # 遍历每个well\n",
    "    for well in wells:\n",
    "        result = meta_data.query(f\"Metadata_Plate == {plate} and Metadata_Well == '{well}'\")\n",
    "        \n",
    "        # 遍历每个site\n",
    "        for site in result[\"Metadata_Site\"].unique():\n",
    "            # 读取 site-level 的所有图像，然后处理成 embedding\n",
    "            img_path = f'/data/boom/cpg0019/broad/training_images/BBBC022/{plate}/{well}/{site}'\n",
    "            img_paths = get_png_filenames(img_path)\n",
    "            if not img_paths:  # 检查是否为空列表\n",
    "                continue\n",
    "            input_img = image_process(img_paths)\n",
    "            if input_img.numel() == 0:  # 检查是否为空张量\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                image_embeddings = model_eval(input_img)\n",
    "            # 改代码：将image_embeddings放到img_path的路径下面，存为 .npy 文件\n",
    "            # features.append(image_embeddings.cpu())\n",
    "\n",
    "            # 将image_embeddings放到img_path的路径下面，存为 .npy 文件\n",
    "            # /data/boom/cpg0019/broad/workspace_dl/embeddings/105281_zenodo7114558/BBBC022/20585/A01/1/\n",
    "            output_path = os.path.join(img_path.replace('training_images','workspace_dl/embeddings/105281_zenodo7114558'), \"Phenom_embeddings.npy\")\n",
    "            np.save(output_path, image_embeddings.cpu().numpy())\n",
    "            \n",
    "            # 清理显存\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "        break\n",
    "    break\n",
    "print(output_path, image_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da35ec1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([114, 384])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ff2e3f",
   "metadata": {},
   "source": [
    "# 3. Test BBBC036 dataset in end to end pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f368c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metadata_Plate</th>\n",
       "      <th>Metadata_Well</th>\n",
       "      <th>Metadata_Site</th>\n",
       "      <th>Plate_Map_Name</th>\n",
       "      <th>DNA</th>\n",
       "      <th>ER</th>\n",
       "      <th>RNA</th>\n",
       "      <th>AGP</th>\n",
       "      <th>Mito</th>\n",
       "      <th>broad_sample_Replicate</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>Compound</th>\n",
       "      <th>Concentration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24277</td>\n",
       "      <td>a01</td>\n",
       "      <td>2</td>\n",
       "      <td>H-BIOA-004-3</td>\n",
       "      <td>24277/cdp2bioactives_a01_s2_w15e4541e6-dfcb-40...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s2_w36b0ca5a6-63c8-44...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s2_w26bec6edf-cec2-45...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s2_w44bf0ab1d-a7a1-42...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s2_w5d2a5a2a2-8548-40...</td>\n",
       "      <td>1</td>\n",
       "      <td>BRD-K18250272@3.02251611288227</td>\n",
       "      <td>BRD-K18250272</td>\n",
       "      <td>3.022516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24277</td>\n",
       "      <td>a01</td>\n",
       "      <td>3</td>\n",
       "      <td>H-BIOA-004-3</td>\n",
       "      <td>24277/cdp2bioactives_a01_s3_w148f1c410-2d16-4c...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s3_w32dc91611-633b-4a...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s3_w29a7efc66-a795-4e...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s3_w49abe4e30-4eed-41...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s3_w5bf626b46-cc60-41...</td>\n",
       "      <td>1</td>\n",
       "      <td>BRD-K18250272@3.02251611288227</td>\n",
       "      <td>BRD-K18250272</td>\n",
       "      <td>3.022516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24277</td>\n",
       "      <td>a01</td>\n",
       "      <td>4</td>\n",
       "      <td>H-BIOA-004-3</td>\n",
       "      <td>24277/cdp2bioactives_a01_s4_w1e9b39acd-2c91-4d...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s4_w394c0f934-4bc1-47...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s4_w2bc33bb34-f4a1-4d...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s4_w41acd7309-157d-41...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s4_w5d76fb6c3-8bf5-43...</td>\n",
       "      <td>1</td>\n",
       "      <td>BRD-K18250272@3.02251611288227</td>\n",
       "      <td>BRD-K18250272</td>\n",
       "      <td>3.022516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24277</td>\n",
       "      <td>a01</td>\n",
       "      <td>5</td>\n",
       "      <td>H-BIOA-004-3</td>\n",
       "      <td>24277/cdp2bioactives_a01_s5_w196a63107-8e9d-41...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s5_w3aa91cc5a-8c46-4a...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s5_w23869d8be-1e44-42...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s5_w4b254a0b2-f029-4b...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s5_w5ae645261-7db0-40...</td>\n",
       "      <td>1</td>\n",
       "      <td>BRD-K18250272@3.02251611288227</td>\n",
       "      <td>BRD-K18250272</td>\n",
       "      <td>3.022516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24277</td>\n",
       "      <td>a01</td>\n",
       "      <td>6</td>\n",
       "      <td>H-BIOA-004-3</td>\n",
       "      <td>24277/cdp2bioactives_a01_s6_w1dcad1406-ee25-41...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s6_w3e008f0a8-1642-44...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s6_w2fa2c1b40-ec9b-4d...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s6_w4b328e14f-5559-4b...</td>\n",
       "      <td>24277/cdp2bioactives_a01_s6_w503cdcfe7-bb4d-4c...</td>\n",
       "      <td>1</td>\n",
       "      <td>BRD-K18250272@3.02251611288227</td>\n",
       "      <td>BRD-K18250272</td>\n",
       "      <td>3.022516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Metadata_Plate Metadata_Well  Metadata_Site Plate_Map_Name  \\\n",
       "0           24277           a01              2   H-BIOA-004-3   \n",
       "1           24277           a01              3   H-BIOA-004-3   \n",
       "2           24277           a01              4   H-BIOA-004-3   \n",
       "3           24277           a01              5   H-BIOA-004-3   \n",
       "4           24277           a01              6   H-BIOA-004-3   \n",
       "\n",
       "                                                 DNA  \\\n",
       "0  24277/cdp2bioactives_a01_s2_w15e4541e6-dfcb-40...   \n",
       "1  24277/cdp2bioactives_a01_s3_w148f1c410-2d16-4c...   \n",
       "2  24277/cdp2bioactives_a01_s4_w1e9b39acd-2c91-4d...   \n",
       "3  24277/cdp2bioactives_a01_s5_w196a63107-8e9d-41...   \n",
       "4  24277/cdp2bioactives_a01_s6_w1dcad1406-ee25-41...   \n",
       "\n",
       "                                                  ER  \\\n",
       "0  24277/cdp2bioactives_a01_s2_w36b0ca5a6-63c8-44...   \n",
       "1  24277/cdp2bioactives_a01_s3_w32dc91611-633b-4a...   \n",
       "2  24277/cdp2bioactives_a01_s4_w394c0f934-4bc1-47...   \n",
       "3  24277/cdp2bioactives_a01_s5_w3aa91cc5a-8c46-4a...   \n",
       "4  24277/cdp2bioactives_a01_s6_w3e008f0a8-1642-44...   \n",
       "\n",
       "                                                 RNA  \\\n",
       "0  24277/cdp2bioactives_a01_s2_w26bec6edf-cec2-45...   \n",
       "1  24277/cdp2bioactives_a01_s3_w29a7efc66-a795-4e...   \n",
       "2  24277/cdp2bioactives_a01_s4_w2bc33bb34-f4a1-4d...   \n",
       "3  24277/cdp2bioactives_a01_s5_w23869d8be-1e44-42...   \n",
       "4  24277/cdp2bioactives_a01_s6_w2fa2c1b40-ec9b-4d...   \n",
       "\n",
       "                                                 AGP  \\\n",
       "0  24277/cdp2bioactives_a01_s2_w44bf0ab1d-a7a1-42...   \n",
       "1  24277/cdp2bioactives_a01_s3_w49abe4e30-4eed-41...   \n",
       "2  24277/cdp2bioactives_a01_s4_w41acd7309-157d-41...   \n",
       "3  24277/cdp2bioactives_a01_s5_w4b254a0b2-f029-4b...   \n",
       "4  24277/cdp2bioactives_a01_s6_w4b328e14f-5559-4b...   \n",
       "\n",
       "                                                Mito  broad_sample_Replicate  \\\n",
       "0  24277/cdp2bioactives_a01_s2_w5d2a5a2a2-8548-40...                       1   \n",
       "1  24277/cdp2bioactives_a01_s3_w5bf626b46-cc60-41...                       1   \n",
       "2  24277/cdp2bioactives_a01_s4_w5d76fb6c3-8bf5-43...                       1   \n",
       "3  24277/cdp2bioactives_a01_s5_w5ae645261-7db0-40...                       1   \n",
       "4  24277/cdp2bioactives_a01_s6_w503cdcfe7-bb4d-4c...                       1   \n",
       "\n",
       "                        Treatment       Compound  Concentration  \n",
       "0  BRD-K18250272@3.02251611288227  BRD-K18250272       3.022516  \n",
       "1  BRD-K18250272@3.02251611288227  BRD-K18250272       3.022516  \n",
       "2  BRD-K18250272@3.02251611288227  BRD-K18250272       3.022516  \n",
       "3  BRD-K18250272@3.02251611288227  BRD-K18250272       3.022516  \n",
       "4  BRD-K18250272@3.02251611288227  BRD-K18250272       3.022516  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = pd.read_csv(os.path.join('/data/boom/cpg0019/broad/workspace_dl/metadata/BBBC036_profiling.csv'))\n",
    "meta_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ea5d48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/55 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/55 [12:31<11:16:01, 751.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 2/55 [25:58<11:32:39, 784.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3/55 [40:04<11:44:00, 812.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4/55 [53:31<11:28:40, 810.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 5/55 [1:07:44<11:28:02, 825.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 6/55 [1:19:17<10:37:25, 780.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 7/55 [1:30:14<9:52:13, 740.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 8/55 [1:42:24<9:37:22, 737.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 9/55 [1:55:00<9:29:34, 742.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 10/55 [2:07:15<9:15:23, 740.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 11/55 [2:20:37<9:16:48, 759.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 12/55 [2:29:20<8:12:35, 687.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 13/55 [2:38:10<7:27:53, 639.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 14/55 [2:47:18<6:58:07, 611.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 15/55 [2:54:09<6:07:40, 551.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 16/55 [3:03:13<5:57:02, 549.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 17/55 [3:09:43<5:17:29, 501.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 18/55 [3:16:31<4:51:54, 473.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 19/55 [3:23:25<4:33:15, 455.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 20/55 [3:31:02<4:25:53, 455.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 21/55 [3:39:22<4:25:45, 469.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 22/55 [3:47:15<4:18:38, 470.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 23/55 [3:56:18<4:22:26, 492.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 24/55 [4:04:16<4:12:03, 487.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 25/55 [4:11:00<3:51:26, 462.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 26/55 [4:18:41<3:43:22, 462.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 27/55 [4:26:48<3:39:09, 469.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 28/55 [4:34:42<3:32:00, 471.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 29/55 [4:48:49<4:12:53, 583.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 30/55 [5:04:46<4:49:53, 695.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 31/55 [5:21:27<5:14:59, 787.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 32/55 [5:42:23<5:55:40, 927.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 33/55 [6:10:46<7:05:33, 1160.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 34/55 [6:23:28<6:04:19, 1040.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 35/55 [6:34:54<5:11:25, 934.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 36/55 [6:46:30<4:33:12, 862.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 37/55 [6:58:58<4:08:34, 828.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 38/55 [7:12:24<3:52:49, 821.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 39/55 [7:25:28<3:36:04, 810.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 40/55 [7:33:12<2:56:37, 706.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 41/55 [7:42:19<2:33:39, 658.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 42/55 [7:51:10<2:14:25, 620.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 43/55 [7:59:28<1:56:45, 583.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 44/55 [8:07:46<1:42:18, 558.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 45/55 [8:13:49<1:23:12, 499.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 46/55 [8:20:00<1:09:07, 460.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 47/55 [8:26:32<58:42, 440.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 48/55 [8:33:45<51:05, 437.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 49/55 [8:40:25<42:40, 426.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 50/55 [8:48:40<37:15, 447.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 51/55 [8:56:26<30:11, 452.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 52/55 [9:02:53<21:39, 433.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 53/55 [9:09:06<13:50, 415.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 54/55 [9:16:27<07:02, 422.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [9:23:35<00:00, 614.82s/it]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 定义超参数\n",
    "save_path = \"../bbbc036/\"\n",
    "\n",
    "# 读取meta文件\n",
    "meta_data = pd.read_csv(os.path.join('/data/boom/cpg0019/broad/workspace_dl/metadata/BBBC036_profiling.csv'))\n",
    "\n",
    "# 针对全部图像，逐个读取，逐个处理单个图像, 返回读取好的图像\n",
    "def image_process(img_paths):\n",
    "    all_img = []\n",
    "    for img_path in img_paths:\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            # 分割大图像为6个子图像，每个子图像的尺寸为（160，160）\n",
    "            sub_images = []\n",
    "            for i in range(6):\n",
    "                left = i * 160\n",
    "                upper = 0\n",
    "                right = left + 160\n",
    "                lower = upper + 160\n",
    "                sub_image = img.crop((left, upper, right, lower))\n",
    "                sub_images.append(sub_image)\n",
    "            \n",
    "            # 按照通道叠加前面5张子图像在一起\n",
    "            combined_image = np.stack(sub_images[:5], axis=0)\n",
    "            resized_image = resize(combined_image, (5, 224, 224), anti_aliasing=True)\n",
    "            all_img.append(resized_image)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {img_path}\")\n",
    "            continue\n",
    "    \n",
    "    # 拼接all_img的所有数据，成为维度(B, 5, 224, 224)\n",
    "    if all_img:\n",
    "        all_img_tensor = torch.tensor(np.array(all_img)).float()\n",
    "        return all_img_tensor\n",
    "    else:\n",
    "        return torch.empty(0)\n",
    "\n",
    "# 加载模型\n",
    "MODEL_PATH = \"recursionpharma/OpenPhenom\"\n",
    "model = MAEModel.from_pretrained(MODEL_PATH).cuda()\n",
    "# img_embeddings = get_image_embeddings(MODEL_PATH, model, batch_size=1)  # change batch_size to fit your device\n",
    "# features = img_embeddings.cpu().numpy()\n",
    "\n",
    "model.eval()\n",
    "print(\"Finished loading model\")\n",
    "\n",
    "def model_eval(batch_imgs):\n",
    "    model = MAEModel.from_pretrained(MODEL_PATH).cuda()\n",
    "    model.return_channelwise_embeddings = False\n",
    "    image_embeddings = model.predict(batch_imgs.cuda())\n",
    "    \n",
    "    return image_embeddings\n",
    "\n",
    "def get_png_filenames(directory):\n",
    "    # 支持的图像文件扩展名\n",
    "    image_extension = '.png'\n",
    "    image_filenames = []\n",
    "\n",
    "    try:\n",
    "        # 遍历目录中的文件（不包括子目录）\n",
    "        for file in os.listdir(directory):\n",
    "            # 检查文件扩展名是否为PNG\n",
    "            if file.lower().endswith(image_extension):\n",
    "                image_filenames.append(os.path.join(directory, file))\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "        # print(f\"Directory not found: {directory}\")\n",
    "    \n",
    "    return image_filenames\n",
    "\n",
    "# 存储所有site的处理结果\n",
    "features = []\n",
    "\n",
    "# 遍历每个plate\n",
    "for plate in tqdm(meta_data[\"Metadata_Plate\"].unique()):\n",
    "    print(plate)\n",
    "    m1 = meta_data[\"Metadata_Plate\"] == plate\n",
    "    wells = meta_data[m1][\"Metadata_Well\"].unique()\n",
    "    \n",
    "    # 遍历每个well\n",
    "    for well in wells:\n",
    "        # print(well)\n",
    "        result = meta_data.query(f\"Metadata_Plate == {plate} and Metadata_Well == '{well}'\")\n",
    "        \n",
    "        # 遍历每个site\n",
    "        for site in result[\"Metadata_Site\"].unique():\n",
    "            # 读取 site-level 的所有图像，然后处理成 embedding\n",
    "            img_path = f'/data/boom/cpg0019/broad/training_images/BBBC036/{plate}/{well}/{site}'\n",
    "            img_paths = get_png_filenames(img_path)\n",
    "            if not img_paths:  # 检查是否为空列表\n",
    "                continue\n",
    "            input_img = image_process(img_paths)\n",
    "            if input_img.numel() == 0:  # 检查是否为空张量\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                image_embeddings = model_eval(input_img)\n",
    "            # 改代码：将image_embeddings放到img_path的路径下面，存为 .npy 文件\n",
    "            # features.append(image_embeddings.cpu())\n",
    "\n",
    "            # 将image_embeddings放到img_path的路径下面，存为 .npy 文件\n",
    "            # /data/boom/cpg0019/broad/workspace_dl/embeddings/105281_zenodo7114558/BBBC022/20585/A01/1/\n",
    "            output_path = os.path.join(img_path.replace('training_images','workspace_dl/embeddings/105281_zenodo7114558'), \"Phenom_embeddings.npy\")\n",
    "            np.save(output_path, image_embeddings.cpu().numpy())\n",
    "            \n",
    "            # 清理显存\n",
    "            torch.cuda.empty_cache()\n",
    "#             break\n",
    "#         break\n",
    "#     break\n",
    "# print(output_path, image_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b15de23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119736, 384)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaeecc8",
   "metadata": {},
   "source": [
    "# 4. Test BBBC037 dataset in end to end pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed90e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 定义超参数\n",
    "save_path = \"../bbbc037/\"\n",
    "\n",
    "# 读取meta文件\n",
    "meta_data = pd.read_csv(os.path.join('/data/boom/cpg0019/broad/workspace_dl/metadata/BBBC037_profiling.csv'))\n",
    "\n",
    "# 针对全部图像，逐个读取，逐个处理单个图像, 返回读取好的图像\n",
    "def image_process(img_paths):\n",
    "    all_img = []\n",
    "    for img_path in img_paths:\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            # 分割大图像为6个子图像，每个子图像的尺寸为（160，160）\n",
    "            sub_images = []\n",
    "            for i in range(6):\n",
    "                left = i * 160\n",
    "                upper = 0\n",
    "                right = left + 160\n",
    "                lower = upper + 160\n",
    "                sub_image = img.crop((left, upper, right, lower))\n",
    "                sub_images.append(sub_image)\n",
    "            \n",
    "            # 按照通道叠加前面5张子图像在一起\n",
    "            combined_image = np.stack(sub_images[:5], axis=0)\n",
    "            resized_image = resize(combined_image, (5, 224, 224), anti_aliasing=True)\n",
    "            all_img.append(resized_image)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {img_path}\")\n",
    "            continue\n",
    "    \n",
    "    # 拼接all_img的所有数据，成为维度(B, 5, 224, 224)\n",
    "    if all_img:\n",
    "        all_img_tensor = torch.tensor(np.array(all_img)).float()\n",
    "        return all_img_tensor\n",
    "    else:\n",
    "        return torch.empty(0)\n",
    "\n",
    "# 加载模型\n",
    "MODEL_PATH = \"recursionpharma/OpenPhenom\"\n",
    "model = MAEModel.from_pretrained(MODEL_PATH).cuda()\n",
    "# img_embeddings = get_image_embeddings(MODEL_PATH, model, batch_size=1)  # change batch_size to fit your device\n",
    "# features = img_embeddings.cpu().numpy()\n",
    "\n",
    "model = MAEModel.from_pretrained(MODEL_PATH).cuda().eval()\n",
    "print(\"Finished loading model\")\n",
    "    \n",
    "def model_eval(batch_imgs):\n",
    "    model.return_channelwise_embeddings = False\n",
    "    image_embeddings = model.predict(batch_imgs.cuda())\n",
    "    \n",
    "    return image_embeddings\n",
    "\n",
    "def get_png_filenames(directory):\n",
    "    # 支持的图像文件扩展名\n",
    "    image_extension = '.png'\n",
    "    image_filenames = []\n",
    "\n",
    "    try:\n",
    "        # 遍历目录中的文件（不包括子目录）\n",
    "        for file in os.listdir(directory):\n",
    "            # 检查文件扩展名是否为PNG\n",
    "            if file.lower().endswith(image_extension):\n",
    "                image_filenames.append(os.path.join(directory, file))\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "        # print(f\"Directory not found: {directory}\")\n",
    "    \n",
    "    return image_filenames\n",
    "\n",
    "# 存储所有site的处理结果\n",
    "features = []\n",
    "\n",
    "# 遍历每个plate\n",
    "for plate in tqdm(meta_data[\"Metadata_Plate\"].unique()):\n",
    "    # print(plate)\n",
    "    m1 = meta_data[\"Metadata_Plate\"] == plate\n",
    "    wells = meta_data[m1][\"Metadata_Well\"].unique()\n",
    "    \n",
    "    # 遍历每个well\n",
    "    for well in wells:\n",
    "        # print(well)\n",
    "        result = meta_data.query(f\"Metadata_Plate == {plate} and Metadata_Well == '{well}'\")\n",
    "        \n",
    "        # 遍历每个site\n",
    "        for site in result[\"Metadata_Site\"].unique():\n",
    "            # 读取 site-level 的所有图像，然后处理成 embedding\n",
    "            img_path = f'/data/boom/cpg0019/broad/training_images/BBBC037/{plate}/{well}/{site}'\n",
    "            img_paths = get_png_filenames(img_path)\n",
    "            if not img_paths:  # 检查是否为空列表\n",
    "                continue\n",
    "            input_img = image_process(img_paths)\n",
    "            if input_img.numel() == 0:  # 检查是否为空张量\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                # print(input_img.shape) [38, 5, 224, 224]\n",
    "                image_embeddings = model_eval(input_img)\n",
    "            # 改代码：将image_embeddings放到img_path的路径下面，存为 .npy 文件\n",
    "            # features.append(image_embeddings.cpu())\n",
    "\n",
    "            # 将image_embeddings放到img_path的路径下面，存为 .npy 文件\n",
    "            # /data/boom/cpg0019/broad/workspace_dl/embeddings/105281_zenodo7114558/BBBC022/20585/A01/1/\n",
    "            output_path = os.path.join(img_path.replace('training_images','workspace_dl/embeddings/105281_zenodo7114558'), \"Phenom_embeddings.npy\")\n",
    "            np.save(output_path, image_embeddings.cpu().numpy())\n",
    "            \n",
    "            # 清理显存\n",
    "            torch.cuda.empty_cache()\n",
    "#             break\n",
    "#         break\n",
    "#     break\n",
    "# print(output_path, image_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f468c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
